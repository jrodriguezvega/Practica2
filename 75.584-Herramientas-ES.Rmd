---
title: 'Practica 2: Limpieza y análisis de datos'
author: 'Juan Rodríguez Vega, Alejandro Gallardo Alberola'
date: "Enero 2021"
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
    includes:
      in_header: PEC-header.html
      after_body: PEC-header.html
  word_document: default
  pdf_train_document:
    highlight: zenburn
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval=T, echo=T)
```


******
# Enunciado
******

El objetivo de esta actividad será el tratamiento de un dataset, que puede ser el creado en la práctica 1 o bien cualquier dataset libre disponible en Kaggle (<https://www.kaggle.com>).

Algunos ejemplos de dataset con los que podéis trabajar son:

- Red Wine Quality (<https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009>)

- Titanic: Machine Learning from Disaster (<https://www.kaggle.com/c/titanic>)

El último ejemplo corresponde a una competición activa de Kaggle de manera que, opcionalmente, podéis aprovechar el trabajo realizado durante la práctica para entrar en esta competición.

Siguiendo las principales etapas de un proyecto analítico, las diferentes tareas a realizar (y justificar) son las siguientes:

1. Descripción del dataset. ¿Por qué es importante y qué pregunta/problema pretende responder?

2. Integración y selección de los datos de interés a analizar.

3. Limpieza de los datos.

  3.1. ¿Los datos contienen ceros o elementos vacíos? ¿Cómo gestionarías cada uno de estos casos?
  
  3.2. Identificación y tratamiento de valores extremos.
  
4. Análisis de los datos.

  4.1. Selección de los grupos de datos que se quieren analizar/comparar (planificación de los análisis a aplicar).
  
  4.2. Comprobación de la normalidad y homogeneidad de la varianza.
  
  4.3. Aplicación de pruebas estadísticas para comparar los grupos de datos. En función de los datos y el objetivo del estudio, aplicar pruebas de contraste de hipótesis,correlaciones, regresiones, etc. Aplicar al menos tres métodos de análisis diferentes.

5. Representación de los resultados a partir de tablas y gráficas.

6. Resolución del problema. A partir de los resultados obtenidos, ¿cuáles son las conclusiones? ¿Los resultados permiten responder al problema?


******
# Solución
******

## Descripción del dataset

El RMS Titanic fue un transatlántico británico, el mayor barco de pasajeros del mundo al finalizar su construcción, que se hundió durante la noche del 14 y la madrugada del 15 de abril de 1912 durante su viaje inaugural desde Southampton a Nueva York. En el hundimiento del Titanic murieron 1496 personas de las 2208 que iban a bordo, lo que convierte a esta catástrofe en uno de los mayores naufragios de la historia ocurridos en tiempo de paz. Construido entre 1909 y 1912 en los astilleros de Harland & Wolff en Belfast, el Titanic era el segundo de los tres buques que formaban la clase Olympic, propiedad de la naviera White Star Line, junto al RMS Olympic y, posteriormente, el HMHS Britannic (Wikipedia).

El conjunto de datos está dividido en dos conjuntos de datos:

- ***train.csv***: conjunto de pasajeros para elaborar los modelos de aprendizaje. Ya están clasificados (o sobrevivieron o murieron).

- ***test.csv***: conjunto de pasajeros no clasificados para aplicar la predicción de los modelos.

Es importante señalar que el dataset no contiene los datos de todos los pasajeros del barco. El dataset de entrenamiento únicamente contiene 891 registros con 12 variables:

- ***PassengerID***: ID unívoco del pasajero

- ***Survived***: el pasajero sobrevive o no (0 = No, 1 = Yes)

- ***Pclass***: Clase del billete (1 = 1st, 2 = 2nd, 3 = 3rd)

- ***Name***: Nombre del pasajero

- ***Sex***: Género del pasajero ("male" o "female")

- ***Age***: Edad del pasajero

- ***AibSp***: nº de hermanos/cónyuges del pasajero abordo del barco

- ***Parch***: nº de padres/hijos del pasajero abordo del barco

- ***Ticket***: nº de billete del pasajero

- ***Fare***: Tarifa del pasajero

- ***Ticket***: nº de billete del pasajero

- ***Cabin***: nº de cabina del pasajero

- ***Embarked***: Puerto de embarque del pasajero (C = Cherbourg, Q = Queenstown, S = Southampton)

A partir de este conjunto de datos se pretende dar respuesta a muchas de las cuestiones que siempre han rodeado a la supervencia de los pasajeros del Titanic. De esta forma, se puede ver qué condiciones (variables) influyeron principalmente a la hora de que una persona sobreviese o no. Por ejemplo:

- ¿Se priorizó a los pasajeros de primera clase sobre el resto?

- ¿Fueron las mujeres y los niños los primeros que embarcaron en los botes salvavidas?

- ¿Qué tipos de pasajero tuvieron más probabilidades de sobrevivir?

- ¿Qué tipo de pasajero procedía de cada punto de embarque?

- Etc.

## Integración y selección de los datos

En este apartado se procede a cargar el dataset *train* y *test*, ya que sobre el primero se construyen los modelos y sobre el segundo se realiza la fase de test.

A continuación, se muestran algunos registros e información general de los datos que servirá para posteriormente proceder a la limpieza a adaptación de los datos.

```{r echo=TRUE, message=FALSE, warning=FALSE}

# Carga de librerías
library(psych)
library(Hmisc)
library(psych)
library(corrplot)


# Carga de los datasets
df_train <- read.csv("train.csv", header=T, sep=",")
df_test <- read.csv("test.csv", header=T, sep=",")

# Visualización de algunos registros del dataset
head(df_train,5)
tail(df_train,5)

# Datos estadísticos básicos
summary(df_train)

# Estructura y tipo de los datos
str(df_train)
```

## Limpieza de los datos

### Análisis de valores vacíos y/o nulos

En primer lugar se realiza un análisis de los valores nulos o vacíos y, posteriormente, se procede a eliminar variables con poco valor significativo para el análisis de datos.

*Nota*: Estos valores están representados en el dataset por NA o "". No aparecen otros típicos como " " o "?".

```{r echo=TRUE, message=FALSE, warning=FALSE}

# Análisis de valores nulos o vacíos
colMeans(is.na(df_train))
colMeans(df_train =="")
```

Tal y como se puede comprobar, la variable *Cabin* posee un 77% de valores nulos o vacíos. Por tanto, se procede a prescindir de dicha variable ya que no tiene sentido inferir tal elevada cantidad de valores. 

Por otro lado, la variable *Age* posee casi un 20% de valores nulos; en este caso concreto, si bien es una cifra considerable, la significancia de este atributo puede ser relevante y, por tanto, se mantiene. En un análisis posterior, se determinará si se infieren estos valores o no y ver cómo afecta a los distintos estadísticos. Por ejemplo, una opción podría ser sustituir los valores por la media/mediana en tanto que exista un comportamiento de normalidad.

Por último, se observa que la variable *Embarked* posee un 0.2% de valores nulos. En este sentido, al tratarse de una variable cualitativa, se procede a inferir dichos valores con el valor más representado.


```{r echo=TRUE, message=FALSE, warning=FALSE}
# Se eliminan las variables Name, Ticket y Cabin
df_train <- df_train[,-c(4, 9, 11)]

# Inferencia de valores nulos más representativo (S - Southampton)
table(df_train$Embarked)
df_train$Embarked <- replace(df_train$Embarked, which(is.na(df_train$Embarked)), 'S')
```

### Conversión y adaptación de los datos

Además de suprimir la variable ***Cabin***, se eliminan del conjunto de datos las variables ***name*** y ***Ticket*** puesto que son identificativos unívocos que no aportan valor y cuya funcionalidad ya viene dada por la variable ***PassengerID***.

Por último, se procede a adaptar los datos para facilitar su posterior análisis y realizando la conversión de tipos de algunas variables.

```{r echo=TRUE, message=FALSE, warning=FALSE}
#Factorización
df_train$Survived <- factor(df_train$Survived, levels = c(0,1), labels= c("No", "Yes"))
df_train$Pclass <- factor(df_train$Pclass, levels = c(1,2,3), labels= c("1st", "2nd", "3rd"))
df_train$Sex <- factor(df_train$Sex, levels= c("female", "male"), labels = c("Female", "Male"))
df_train$Embarked <- factor(df_train$Sex, levels= c("C","Q","S"), labels = c("Cherbourg","Queenstown", "Southampton"))

```

### Análisis de valores extremos

A continuación, se procede a visualizar y analizar los posibles *outliers* asociados a las variables contínuas.

```{r echo=TRUE, message=FALSE, warning=FALSE}

# Index. de variables cuantitativas
v_var_cuant <- c(5:8)

# Estadísticas de valores nulos o vacíos
par(mfrow=c(1,4))
for(i in v_var_cuant){
   boxplot(df_train[,i], main=colnames(df_train)[i])
}
```

Es importante considerar que estas observaciones pueden afectar a los estadísticos y, por tanto, hacer análisis sesgado de los datos (por ejemplo, incrementan significativamente el error en la varianza de los datos). Se puede observar algunas muestras fuera de lo común, si bien, no parecen cifras muy desproporcionadas. Por tanto, se tendrán en cuenta para un primer análisis.

*Nota:* En análisis posteriores es interesante ver cómo afectan estos datos a los estadísticos y tomar otro tipo de decisiones sobre qué hacer con estos.

## Análisis de datos

### Medidas de dispersión

Se analizan otros estadísticos como la varianza o la desviación estándar y se aplica el test de Shapiro-Wilk para comprobar la normalidad de las variables a través de contraste de hipótesis y, a partir del cual, si el p-value es menor al nivel de significancia (0.05), se rechaza la hipótesis nula (distribución normal). 



```{r echo=TRUE, message=FALSE, warning=FALSE}
v_var <- vector(length = length(v_var_cuant))
v_sd <- vector(length = length(v_var_cuant))
v_pvalue_shapiro <- vector(length = length(v_var_cuant))
for (i in seq_along(v_var_cuant)){
  v_var[i] <- var(df_train[,v_var_cuant[i]],na.rm = TRUE)
  v_sd[i] <- sd(df_train[,v_var_cuant[i]] ,na.rm = TRUE)
  v_pvalue_shapiro[i] <- as.double(shapiro.test(df_train[,v_var_cuant[i]])["p.value"])
}
medidas_dispersion_shap <- data.frame(colnames(df_train[,v_var_cuant]),round(v_var,4), round(v_sd,4), round(v_pvalue_shapiro,4))

colnames(medidas_dispersion_shap) <- c("attr", "varianza", "desviación estándar", "shapiro p-value")
medidas_dispersion_shap
```


Por tanto, se puede concluir que las variables no atienden a una distribución normal. A continuación, se muestra la distribución de las variables en comparación con la normal donde se verifica lo anteriormente expuesto. Además, se muestra un Q-Q plot para comprobar si los cuantiles o no siguen una distribución lineal. No obstante, puesto que hay un número considerable de muestras, en algunos casos podría considerarse normalidad a través del Teorema del Límite Central (quizá únicamente en la variable *age*).

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Histograma vs. normal
multi.hist(x = df_train[,v_var_cuant], dcol = c("blue", "red"), dlty = c("dotted", "solid"))

# Gráfico Q-Q
par(mfrow = c(1, 4))
for (i in v_var_cuant){
  qqnorm(df_train[,i], main=colnames(df)[i], col = "red")
  qqline(df_train[,i])
}
```

### Homocedasticidad

Puesto que los datos no siguen una distribución normal, se aplicará el test de Fligner-Killeen como alternativa no paramétrica para evaluar la igual de varianzas. La hipótesis nula asume igualdad de varianzas en los diferentes grupos de datos (homocedasticidad), por lo que p-valores inferiores al nivel de significancia indicarán heterocedasticidad.


```{r echo=TRUE, message=FALSE, warning=FALSE}

fligner.test(Age ~ SibSp, data = df_train)
fligner.test(Age ~ Parch, data = df_train)
fligner.test(Age ~ Fare, data = df_train)
fligner.test(SibSp ~ Parch, data = df_train)
fligner.test(SibSp ~ Fare, data = df_train)
fligner.test(Parch ~ Fare, data = df_train)
```

Se verifica que todas las variables dos a dos cumplen heterocedasticidad al tener todas un variable de p-value inferior al nivel de significancia (0.05).

### Correlación

A continuación, se procede a analizar la correlación entre las variables, para lo que se hace necesario normalizar las variables. En este apartado, conociendo que la variable *age* tiende a la normalidad, se procede a inferir los valores NA por la mediana (más robusta que la media). 

En cualquier caso, se aplica el test de correlación de Spearman, especialmente para casos que no cumplen normalidad.

*Nota*: Se puede comprobar que la inferencia no afecta prácticamente a las medidas de tendencia central (por su normalidad) y las de dispersión se ven ligeramente reducidas.

```{r echo=TRUE, message=FALSE, warning=FALSE}

# Inferencia de valores nulos en Age 
df_train$Age <- replace(df_train$Age, which(is.na(df_train$Age)), median(df_train$Age, na.rm=TRUE))

# Normalización
df_norm <- as.data.frame(scale(df_train[,v_var_cuant]))

# Matriz de correlación y p-value
rcorr(as.matrix(df_norm), type = "spearman")

# Visualización de correlación y los diagramas de dispersión
corrplot.mixed(cor(df_norm, method = "spearman"))
pairs.panels(x = df_norm, ellipses = FALSE, lm = TRUE, method = "spearman", hist.col = "cadetblue1")
```

Se observa que no existen fuertes correlaciones entre las variables cuantitativas (si se establece como umbral |0.5|). Además, se comprueba que son estadísticamente significativos.